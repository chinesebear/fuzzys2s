{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from loguru import logger\n",
    "import tqdm\n",
    "\n",
    "class Options:\n",
    "    def __init__(self, name) -> None:\n",
    "        self.name= name\n",
    "    def name(self):\n",
    "        return self.name\n",
    "\n",
    "# project gloal parameter\n",
    "options = Options(\"Model\")\n",
    "options.base_path=\"/home/yang/github/fuzzys2s/\"\n",
    "options.SOS = 0 # start of sentence\n",
    "options.EOS = 1 # End of sentence\n",
    "options.PAD = 2 # padding token\n",
    "options.UNK = 3 # unknown token, word frequency low\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<sos>\":options.SOS, \"<eos>\":options.EOS, \"<pad>\":options.PAD,\"<unk>\":options.UNK}\n",
    "        self.word2count = {\"<sos>\":1, \"<eos>\":1, \"<pad>\":1,\"<unk>\":1}\n",
    "        self.index2word = {options.SOS: \"<sos>\", options.EOS: \"<eos>\", options.PAD:\"<pad>\",options.UNK: \"<unk>\"}\n",
    "        self.n_words = 4  # Count PAD , SOS and EOS\n",
    "        self.feature_max = [] # max value of feature\n",
    "        self.feature_min = [] # min value of feature\n",
    "        self.line_max = 0 # max length of sentence\n",
    "\n",
    "    def addTokens(self, tokens):\n",
    "        for word in tokens:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"<sos>\":options.SOS, \"<eos>\":options.EOS, \"<pad>\":options.PAD,\"<unk>\":options.UNK}\n",
    "        self.word2count = {\"<sos>\":1, \"<eos>\":1, \"<pad>\":1,\"<unk>\":1}\n",
    "        self.index2word = {options.SOS: \"<sos>\", options.EOS: \"<eos>\", options.PAD:\"<pad>\",options.UNK: \"<unk>\"}\n",
    "        self.n_words = 4  # Count PAD , SOS and EOS\n",
    "        self.feature_max = [] # max value of feature\n",
    "        self.feature_min = [] # min value of feature\n",
    "        self.line_max = 0 # max length of sentence\n",
    "\n",
    "    def addTokens(self, tokens):\n",
    "        for word in tokens:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def get_base_tokenizer(name):\n",
    "    tokenizer_path = options.base_path+\"output/\"+name+\"/\"\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        tokenizer.save_pretrained(tokenizer_path)\n",
    "    return tokenizer.tokenize\n",
    "\n",
    "def read_line_pair(src_path, tgt_path):\n",
    "    src_fd = open(src_path, \"r\")\n",
    "    tgt_fd = open(tgt_path, \"r\")\n",
    "    src_lines = src_fd.readlines()\n",
    "    tgt_lines = tgt_fd.readlines()\n",
    "    lines =[]\n",
    "    for i  in range(len(src_lines)):\n",
    "        src = src_lines[i]\n",
    "        tgt = tgt_lines[i]\n",
    "        lines.append([src, tgt])\n",
    "    src_fd.close()\n",
    "    tgt_fd.close()\n",
    "    return lines\n",
    "\n",
    "def read_dataset(name, subpath):\n",
    "    dataset_path = options.base_path+\"output/\"+name+\"/\"+subpath+\"/\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        dataset = load_from_disk(dataset_path)\n",
    "    else :\n",
    "        if subpath == '':\n",
    "            dataset = load_dataset(name)\n",
    "        else:\n",
    "            dataset = load_dataset(name,subpath)\n",
    "        dataset.save_to_disk(dataset_path)\n",
    "    print(name,'-',subpath,'done')\n",
    "\n",
    "def read_hearthstone_data(tokenizer, vocab):\n",
    "    logger.info(\"read raw data\")\n",
    "    train_lines = read_line_pair(options.base_path+'doc/hearthstone/train_hs.in', options.base_path+'doc/hearthstone/train_hs.out')\n",
    "    valid_lines = read_line_pair(options.base_path+'doc/hearthstone/dev_hs.in', options.base_path+'doc/hearthstone/dev_hs.out')\n",
    "    test_lines = read_line_pair(options.base_path+'doc/hearthstone/test_hs.in', options.base_path+'doc/hearthstone/test_hs.out')\n",
    "    lines = train_lines + valid_lines + test_lines\n",
    "    data = []\n",
    "    for src, tgt in lines:\n",
    "        src_tokens = tokenizer(src)\n",
    "        tgt_tokens = tokenizer(tgt)\n",
    "        vocab.addTokens(src_tokens)\n",
    "        vocab.addTokens(tgt_tokens)\n",
    "        data.append([src_tokens, tgt_tokens])\n",
    "    return data\n",
    "\n",
    "def read_euconst_data(tokenizer, vocab):\n",
    "    logger.info(\"read opus_euconst data\")\n",
    "    dataset = read_dataset('opus_euconst', 'en-fr')\n",
    "    logger.info(\"read raw tokens\")\n",
    "    src_lang = 'en'\n",
    "    tgt_lang = 'fr'\n",
    "    train_len = dataset['train'].num_rows\n",
    "    logger.info(\"dataset:opus_euconst, total: %d\"  %(train_len))\n",
    "    train_raw_data = dataset['train']\n",
    "    train_iter = iter(train_raw_data)\n",
    "    train_data = np.empty([train_len], dtype=int).tolist()\n",
    "    for i in range(train_len):\n",
    "        data = next(train_iter)\n",
    "        src = data['translation'][src_lang]\n",
    "        tgt = data['translation'][tgt_lang]\n",
    "        src = tokenizer(src)\n",
    "        tgt = tokenizer(tgt)\n",
    "        vocab.addTokens(src)\n",
    "        vocab.addTokens(tgt)\n",
    "        train_data[i] = [src, tgt]\n",
    "    return train_data\n",
    "\n",
    "def read_xlsum_data(tokenizer, vocab):\n",
    "    dataset = read_dataset('GEM/xlsum', 'french')\n",
    "    logger.info(\"read raw tokens\")\n",
    "    train_len = dataset['train'].num_rows\n",
    "    valid_len = dataset['validation'].num_rows\n",
    "    test_len = dataset['test'].num_rows\n",
    "    train_data = np.empty([train_len], dtype=int).tolist()\n",
    "    train_iter = iter(dataset['train'])\n",
    "    for i in tqdm(range(train_len), 'read train data'):\n",
    "        data = next(train_iter)\n",
    "        src = data['text']\n",
    "        tgt = data['target']\n",
    "        src = tokenizer(src)\n",
    "        tgt = tokenizer(tgt)\n",
    "        train_data[i] = [src, tgt]\n",
    "    valid_data = np.empty([valid_len], dtype=int).tolist()\n",
    "    valid_iter = iter(dataset['validation'])\n",
    "    for i in tqdm(range(valid_len), 'read valid data'):\n",
    "        data = next(valid_iter)\n",
    "        src = data['text']\n",
    "        tgt = data['target']\n",
    "        src = tokenizer(src)\n",
    "        tgt = tokenizer(tgt)\n",
    "        valid_data[i] = [src, tgt]\n",
    "    test_data = np.empty([test_len], dtype=int).tolist()\n",
    "    test_iter = iter(dataset['test'])\n",
    "    for i in tqdm(range(test_len), 'read test data'):\n",
    "        data = next(test_iter)\n",
    "        src = data['text']\n",
    "        tgt = data['target']\n",
    "        src = tokenizer(src)\n",
    "        tgt = tokenizer(tgt)\n",
    "        test_data[i] = [src, tgt]\n",
    "    data = train_data+ valid_data+ test_data\n",
    "    return data\n",
    "hs_vocab = Vocab('hs')\n",
    "euconst_vocab = Vocab('euconst')\n",
    "xlsum_vocab = Vocab('xlsum')\n",
    "\n",
    "hs_data = read_hearthstone_data(tokenizer, hs_vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
