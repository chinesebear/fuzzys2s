{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN\n",
    "Fuzzy neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "import csv\n",
    "from fcmeans import FCM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data\n",
    "dataset preprocess: Rescaling (min-max normalization„ÄÅrange scaling)\n",
    "\n",
    "https://cloud.tencent.com/developer/article/1803086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  48792 test_data:  50\n",
      "head:  [['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week'], 'target']\n",
      "data : [[0.3013698630136986, 0.875, 0.044131207652990466, 0.6, 0.8, 0.6666666666666666, 0.07142857142857142, 0.2, 1.0, 1.0, 0.021740217402174022, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.4520547945205479, 0.75, 0.048051741576264365, 0.6, 0.8, 0.3333333333333333, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 0.12244897959183673], 1]\n",
      "data : [[0.2876712328767123, 0.5, 0.13758131133233883, 0.7333333333333333, 0.5333333333333333, 0.0, 0.42857142857142855, 0.2, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.4931506849315068, 0.5, 0.15048626121783487, 0.06666666666666667, 0.4, 0.3333333333333333, 0.42857142857142855, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.1506849315068493, 0.5, 0.2206350656072092, 0.6, 0.8, 0.3333333333333333, 0.7142857142857143, 1.0, 0.5, 0.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.273972602739726, 0.5, 0.18421908985430768, 0.8, 0.8666666666666667, 0.3333333333333333, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.4383561643835616, 0.5, 0.10006122662986304, 0.4, 0.26666666666666666, 0.5, 0.5714285714285714, 0.2, 0.5, 0.0, 0.0, 0.0, 0.15306122448979592], 1]\n",
      "data : [[0.4794520547945205, 0.75, 0.13351938110363537, 0.7333333333333333, 0.5333333333333333, 0.3333333333333333, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4489795918367347], 0]\n",
      "data : [[0.1917808219178082, 0.5, 0.022661294960135036, 0.8, 0.8666666666666667, 0.6666666666666666, 0.7142857142857143, 0.2, 1.0, 0.0, 0.14084140841408413, 0.0, 0.5], 0]\n",
      "data : [[0.3424657534246575, 0.5, 0.09956194206810702, 0.6, 0.8, 0.3333333333333333, 0.2857142857142857, 0.0, 1.0, 1.0, 0.051780517805178054, 0.0, 0.3979591836734694], 0]\n",
      "data : [[0.273972602739726, 0.5, 0.18143310906120294, 1.0, 0.6, 0.3333333333333333, 0.2857142857142857, 0.0, 0.5, 1.0, 0.0, 0.0, 0.8061224489795918], 0]\n",
      "data : [[0.1780821917808219, 0.875, 0.08728143615347926, 0.6, 0.8, 0.3333333333333333, 0.7142857142857143, 0.0, 0.25, 1.0, 0.0, 0.0, 0.3979591836734694], 0]\n",
      "data : [[0.0821917808219178, 0.5, 0.07441031313531085, 0.6, 0.8, 0.6666666666666666, 0.07142857142857142, 0.6, 1.0, 0.0, 0.0, 0.0, 0.29591836734693877], 1]\n",
      "data : [[0.2054794520547945, 0.5, 0.13039174895052144, 0.4666666666666667, 0.7333333333333333, 0.6666666666666666, 0.8571428571428571, 0.2, 0.5, 1.0, 0.0, 0.0, 0.5], 1]\n",
      "data : [[0.3150684931506849, 0.5, 0.07407204446203441, 0.5333333333333333, 0.6666666666666666, 0.3333333333333333, 0.21428571428571427, 0.0, 0.25, 1.0, 0.0, 0.0, 0.3979591836734694], 0]\n",
      "data : [[0.2328767123287671, 0.5, 0.1577698622908231, 0.3333333333333333, 0.2, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4489795918367347], 1]\n",
      "data : [[0.1095890410958904, 0.75, 0.11127077392489759, 0.7333333333333333, 0.5333333333333333, 0.6666666666666666, 0.35714285714285715, 0.6, 1.0, 1.0, 0.0, 0.0, 0.3469387755102041], 1]\n",
      "data : [[0.2054794520547945, 0.5, 0.11808215192999191, 0.7333333333333333, 0.5333333333333333, 0.6666666666666666, 0.5, 0.8, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.2876712328767123, 0.5, 0.0112318730274708, 0.06666666666666667, 0.4, 0.3333333333333333, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5], 1]\n",
      "data : [[0.3561643835616438, 0.75, 0.18935603792668365, 0.8, 0.8666666666666667, 0.0, 0.2857142857142857, 0.8, 1.0, 0.0, 0.0, 0.0, 0.4489795918367347], 0]\n",
      "data : [[0.3150684931506849, 0.5, 0.12261495215189616, 0.6666666666666666, 1.0, 0.3333333333333333, 0.7142857142857143, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6020408163265306], 0]\n",
      "data : [[0.5068493150684932, 0.5, 0.19610179180916235, 0.7333333333333333, 0.5333333333333333, 0.8333333333333334, 0.5714285714285714, 0.8, 0.5, 0.0, 0.0, 0.0, 0.19387755102040816], 1]\n",
      "data : [[0.2465753424657534, 0.125, 0.04367725109345349, 0.4, 0.26666666666666666, 0.3333333333333333, 0.35714285714285715, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.3561643835616438, 0.5, 0.07086864012610657, 0.06666666666666667, 0.4, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4687786960514233, 0.3979591836734694], 1]\n",
      "data : [[0.5753424657534246, 0.5, 0.06544145753205942, 0.7333333333333333, 0.5333333333333333, 0.0, 0.9285714285714286, 0.8, 1.0, 0.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.5342465753424658, 0.25, 0.13839653883493502, 0.6, 0.8, 0.3333333333333333, 0.9285714285714286, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 0]\n",
      "data : [[0.0273972602739726, 0.5, 0.10554591489836718, 0.7333333333333333, 0.5333333333333333, 0.6666666666666666, 0.21428571428571427, 0.6, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.5068493150684932, 0.0, 0.11360821045723776, 1.0, 0.6, 0.3333333333333333, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.6020408163265306], 0]\n",
      "data : [[0.3013698630136986, 0.5, 0.24015384459260614, 0.7333333333333333, 0.5333333333333333, 0.0, 0.2857142857142857, 0.2, 1.0, 1.0, 0.0, 0.0, 0.8061224489795918], 1]\n",
      "data : [[0.4383561643835616, 0.5, 0.1225080592511408, 0.7333333333333333, 0.5333333333333333, 0.3333333333333333, 0.21428571428571427, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.0821917808219178, 0.25, 0.12071049952134982, 0.4666666666666667, 0.7333333333333333, 0.6666666666666666, 0.7857142857142857, 0.2, 1.0, 1.0, 0.0, 0.0, 0.5204081632653061], 1]\n",
      "data : [[0.0410958904109589, 0.5, 0.1716578209408605, 1.0, 0.6, 0.6666666666666666, 0.8571428571428571, 0.6, 0.5, 1.0, 0.0, 0.0, 0.4387755102040816], 1]\n",
      "data : [[0.3835616438356164, 0.5, 0.25346809957276667, 0.6, 0.8, 0.0, 0.2857142857142857, 0.6, 1.0, 1.0, 0.0, 0.32323232323232326, 0.3979591836734694], 1]\n",
      "data : [[0.1780821917808219, 0.125, 0.03224782916078925, 1.0, 0.6, 0.3333333333333333, 0.07142857142857142, 0.6, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.0684931506849315, 0.875, 0.20243824059697654, 1.0, 0.6, 0.3333333333333333, 0.5714285714285714, 0.0, 0.5, 1.0, 0.0, 0.0, 0.14285714285714285], 1]\n",
      "data : [[0.4246575342465753, 0.5, 0.15568545072609372, 0.06666666666666667, 0.4, 0.6666666666666666, 0.5, 0.8, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.0547945205479452, 0.5, 0.1251019034378245, 1.0, 0.6, 0.6666666666666666, 0.5, 0.6, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.0273972602739726, 0.5, 0.35978662012089724, 0.7333333333333333, 0.5333333333333333, 0.16666666666666666, 0.07142857142857142, 1.0, 1.0, 0.0, 0.0, 0.0, 0.24489795918367346], 1]\n",
      "data : [[0.1917808219178082, 0.5, 0.04862206255940844, 1.0, 0.6, 0.3333333333333333, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.37755102040816324], 0]\n",
      "data : [[0.4246575342465753, 0.75, 0.17129384384841503, 0.4666666666666667, 0.7333333333333333, 0.3333333333333333, 0.7142857142857143, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.1917808219178082, 0.5, 0.3352851435781384, 0.4, 0.26666666666666666, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855], 1]\n",
      "data : [[0.4931506849315068, 0.75, 0.05156635309160654, 0.6, 0.8, 0.3333333333333333, 0.7142857142857143, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.0958904109589041, 0.5, 0.1087209046657398, 0.6, 0.8, 0.3333333333333333, 0.9285714285714286, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5], 1]\n",
      "data : [[0.4383561643835616, 0.5, 0.05571488010066876, 0.7333333333333333, 0.5333333333333333, 0.8333333333333334, 0.07142857142857142, 0.8, 1.0, 0.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.1095890410958904, 0.5, 0.1878710384510001, 0.7333333333333333, 0.5333333333333333, 0.6666666666666666, 0.42857142857142855, 0.2, 1.0, 1.0, 0.0, 0.0, 0.3469387755102041], 1]\n",
      "data : [[0.547945205479452, 0.125, 0.220287325411081, 0.6, 0.8, 0.3333333333333333, 0.7142857142857143, 0.0, 0.5, 1.0, 0.0, 0.0, 0.3979591836734694], 0]\n",
      "data : [[0.4931506849315068, 0.5, 0.08935434658331727, 0.7333333333333333, 0.5333333333333333, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.37755102040816324], 1]\n",
      "data : [[0.3698630136986301, 0.5, 0.07852501327704543, 0.8, 0.8666666666666667, 0.0, 0.2857142857142857, 0.8, 1.0, 0.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.3287671232876712, 0.875, 0.06042696271940952, 0.5333333333333333, 0.6666666666666666, 0.3333333333333333, 0.21428571428571427, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3979591836734694], 1]\n",
      "data : [[0.1643835616438356, 0.5, 0.1753456260169202, 0.5333333333333333, 0.6666666666666666, 0.6666666666666666, 0.7142857142857143, 0.2, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855], 1]\n"
     ]
    }
   ],
   "source": [
    "# map range: [a, b]\n",
    "def rescaling(x, a, b):\n",
    "    features = [i[0] for i in x]\n",
    "    features = np.array(features)\n",
    "    col_max = features.max(axis=0).tolist() # max of col\n",
    "    col_min = features.min(axis=0).tolist() # min of col\n",
    "    data_num = len(x)\n",
    "    feature_num = len(features[0])\n",
    "    for i in range(data_num):\n",
    "        for j in range(feature_num):\n",
    "            val = x[i][0][j]\n",
    "            val = a + (val - col_min[j])*(b-a) /(col_max[j] - col_min[j])\n",
    "            x[i][0][j] = val\n",
    "    return x\n",
    "    \n",
    "def read_data():\n",
    "    with open('adult.tsv', 'r', encoding='utf-8') as file_obj:\n",
    "        lines_obj = csv.reader(file_obj,delimiter='\\t')\n",
    "        line_list =[]\n",
    "        for line in lines_obj:\n",
    "            if line[0] != 'age':\n",
    "                for i in range(len(line)):\n",
    "                    line[i] = float(line[i])\n",
    "                line[-1] = int(line[-1])\n",
    "            features = line[0:-2]\n",
    "            target = line[-1]\n",
    "            line_list.append([features, target])\n",
    "        head = line_list[0]\n",
    "        train_data = line_list[1:-50]\n",
    "        train_len = len(train_data)\n",
    "        test_data = line_list[-50:]\n",
    "        test_len = len(test_data)\n",
    "        train_data = rescaling(train_data,0,1)\n",
    "        test_data = rescaling(test_data,0,1)\n",
    "    return head,train_data, train_len, test_data, test_len\n",
    "head,train_data, train_len, test_data, test_len= read_data()\n",
    "print(\"train: \", train_len, \"test_data: \", test_len)\n",
    "print(\"head: \",head)\n",
    "for d in train_data[0:50]:\n",
    "    print(\"data :\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCM\n",
    "find center and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.2534, 0.4664, 0.1179, 0.7054, 0.6037, 0.4995, 0.4401, 0.5657, 0.9012,\n",
      "         0.0947, 0.0062, 0.0131, 0.3635],\n",
      "        [0.3112, 0.4951, 0.1205, 0.6861, 0.6100, 0.4100, 0.4821, 0.1715, 0.9323,\n",
      "         0.9006, 0.0118, 0.0216, 0.4197],\n",
      "        [0.2534, 0.4664, 0.1179, 0.7054, 0.6037, 0.4995, 0.4401, 0.5657, 0.9012,\n",
      "         0.0947, 0.0062, 0.0131, 0.3635],\n",
      "        [0.3112, 0.4951, 0.1205, 0.6861, 0.6100, 0.4100, 0.4821, 0.1715, 0.9323,\n",
      "         0.9006, 0.0118, 0.0216, 0.4197],\n",
      "        [0.3112, 0.4951, 0.1205, 0.6861, 0.6100, 0.4100, 0.4821, 0.1715, 0.9323,\n",
      "         0.9006, 0.0118, 0.0216, 0.4197],\n",
      "        [0.3112, 0.4951, 0.1205, 0.6861, 0.6100, 0.4100, 0.4821, 0.1715, 0.9323,\n",
      "         0.9006, 0.0118, 0.0216, 0.4197]], dtype=torch.float64), tensor([[0.3715, 0.3542, 0.0502, 0.7116, 0.2716, 0.8437, 0.9586, 1.1669, 0.5200,\n",
      "         2.4955, 0.0483, 0.0722, 0.1610],\n",
      "        [0.3433, 0.3265, 0.0514, 0.6511, 0.3025, 0.5417, 0.8939, 0.8410, 0.4176,\n",
      "         1.5930, 0.0584, 0.0908, 0.1566],\n",
      "        [0.3715, 0.3542, 0.0502, 0.7116, 0.2716, 0.8437, 0.9586, 1.1669, 0.5200,\n",
      "         2.4955, 0.0483, 0.0722, 0.1610],\n",
      "        [0.3433, 0.3265, 0.0514, 0.6511, 0.3025, 0.5417, 0.8939, 0.8410, 0.4176,\n",
      "         1.5930, 0.0584, 0.0908, 0.1566],\n",
      "        [0.3433, 0.3265, 0.0514, 0.6511, 0.3025, 0.5417, 0.8939, 0.8410, 0.4176,\n",
      "         1.5930, 0.0584, 0.0908, 0.1566],\n",
      "        [0.3433, 0.3265, 0.0514, 0.6511, 0.3025, 0.5417, 0.8939, 0.8410, 0.4176,\n",
      "         1.5930, 0.0584, 0.0908, 0.1566]], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fcm(data, cluster_num, h):\n",
    "    fcm = FCM(n_clusters=cluster_num)\n",
    "    data = [i[0] for i in data]\n",
    "    data = np.array(data)\n",
    "    fcm.fit(data)\n",
    "    centers = fcm.centers\n",
    "    membership = fcm.soft_predict(data)\n",
    "    feature_num = len(data[0])\n",
    "    data_num = len(data)\n",
    "    sigma = []\n",
    "    for i in range(cluster_num):\n",
    "        feature_sigma = []\n",
    "        for j in range(feature_num):\n",
    "            a = 0\n",
    "            b = 0\n",
    "            for k in  range(data_num):\n",
    "                x = data[k][j]\n",
    "                a = a + membership[k][i]* ((x-centers[i][j]) ** 2)\n",
    "                b = b + membership[k][i]\n",
    "            feature_sigma.append(h*a/b)\n",
    "        sigma.append(feature_sigma)\n",
    "    return torch.tensor(centers),torch.tensor(sigma)\n",
    "h = 10\n",
    "print(fcm(train_data, 6, h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MemberShip(nn.Module):\n",
    "    def __init__(self, feature_num, rule_num, center, sigma):\n",
    "        super(MemberShip, self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.rule_num = rule_num\n",
    "        self.center = nn.Parameter(center)\n",
    "        self.sigma = nn.Parameter(sigma)\n",
    "    def forward(self, x):\n",
    "        mb_list = []\n",
    "        for i in range(self.rule_num):\n",
    "            tmp_list = []\n",
    "            for j in range(self.feature_num): \n",
    "                value = ((x[j]-self.center[i][j]) / self.sigma[i][j]) ** 2\n",
    "                value = torch.exp(-(value /2))\n",
    "                tmp_list.append(value)\n",
    "            mb_list.append(tmp_list)\n",
    "        membership = torch.tensor(mb_list).float()\n",
    "        return membership\n",
    "\n",
    "class RuleGen(nn.Module):\n",
    "    def __init__(self,rule_num):\n",
    "        super(RuleGen, self).__init__()\n",
    "        self.rule_num = rule_num\n",
    "    def forward(self, membership):\n",
    "        # x: membership array\n",
    "        # rule_num * feature_num \n",
    "        rule_list = []\n",
    "        feature_num = len(membership[0])\n",
    "        rule_num = self.rule_num \n",
    "        for i in range(rule_num):\n",
    "            value = 1.0\n",
    "            for j in range(feature_num):\n",
    "                value = value * membership[i][j]\n",
    "            rule_list.append(value)\n",
    "        rule = torch.tensor(rule_list).float()\n",
    "        return rule\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self,feature_num , rule_num, center,sigma ):\n",
    "        super(FNN, self).__init__()\n",
    "        self.rule_num = rule_num\n",
    "        self.ms_layer = MemberShip(feature_num, rule_num, center,sigma )\n",
    "        self.rule_layer = RuleGen(rule_num)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(rule_num,1024)\n",
    "        self.fc2 = nn.Linear(1024,2)\n",
    "    def norm(self, x):\n",
    "        sum_x = torch.sum(x, dim=-1)\n",
    "        if sum_x == 0.0:\n",
    "            return x\n",
    "        return x/sum_x\n",
    "    def forward(self, x):\n",
    "        membership = self.ms_layer(x)\n",
    "        rule = self.rule_layer(membership)\n",
    "        rule = self.norm(rule)\n",
    "        output = self.fc(rule)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,feature_num , rule_num, center,sigma ):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Linear(feature_num, 2048)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_out= nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop_out2= nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(1024, 2)\n",
    "    def forward(self, x):\n",
    "        input = self.fc(x)\n",
    "        input = self.relu(input)\n",
    "        input = self.drop_out(input)\n",
    "        input = self.fc2(input)\n",
    "        input = self.relu2(input)\n",
    "        input = self.drop_out2(input)\n",
    "        output = self.fc3(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model total parameters: 2128898, trainable  parameters: 2128898 \n",
      "acc: 38\n",
      "epoch:  0 count:  100 , train loss:  0.9438722412951757 , test loss:  0.522284744232893 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  200 , train loss:  0.7218370665289694 , test loss:  0.512912072353065 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  300 , train loss:  0.6708282938075718 , test loss:  0.49017260571941734 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  400 , train loss:  0.63399121685914 , test loss:  0.48561472378671167 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  500 , train loss:  0.5932356092568952 , test loss:  0.6176979295921047 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  600 , train loss:  0.5758421517843332 , test loss:  0.47903116662055256 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  700 , train loss:  0.5526620390155378 , test loss:  0.45427639373927375 ,acc:  0.84\n",
      "acc: 38\n",
      "epoch:  0 count:  800 , train loss:  0.5434223925130754 , test loss:  0.40144343370979185 ,acc:  0.76\n",
      "acc: 43\n",
      "epoch:  0 count:  900 , train loss:  0.5341365084839022 , test loss:  0.4172282318846555 ,acc:  0.86\n",
      "acc: 46\n",
      "epoch:  0 count:  1000 , train loss:  0.5236454374698951 , test loss:  0.43310855238407386 ,acc:  0.92\n",
      "acc: 45\n",
      "epoch:  0 count:  1100 , train loss:  0.5169242082028888 , test loss:  0.38776549599278953 ,acc:  0.9\n",
      "acc: 41\n",
      "epoch:  0 count:  1200 , train loss:  0.5135802508955264 , test loss:  0.4099697286769515 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  1300 , train loss:  0.5126708986227458 , test loss:  0.4053813669556985 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  1400 , train loss:  0.5140005561981213 , test loss:  0.4332241670059739 ,acc:  0.82\n",
      "acc: 37\n",
      "epoch:  0 count:  1500 , train loss:  0.5134419279442227 , test loss:  0.4392233297461644 ,acc:  0.74\n",
      "acc: 42\n",
      "epoch:  0 count:  1600 , train loss:  0.5071540128441757 , test loss:  0.389965469641611 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  1700 , train loss:  0.5014957189652983 , test loss:  0.36180413465241146 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  1800 , train loss:  0.4944222118650012 , test loss:  0.3713157462252275 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  1900 , train loss:  0.4904551752850047 , test loss:  0.41098353484412653 ,acc:  0.88\n",
      "acc: 44\n",
      "epoch:  0 count:  2000 , train loss:  0.48819148650768696 , test loss:  0.4179479271452874 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  0 count:  2100 , train loss:  0.48397831435968414 , test loss:  0.3835865519379149 ,acc:  0.84\n",
      "acc: 45\n",
      "epoch:  0 count:  2200 , train loss:  0.48192649373227225 , test loss:  0.33299394642119295 ,acc:  0.9\n",
      "acc: 45\n",
      "epoch:  0 count:  2300 , train loss:  0.4784842879974879 , test loss:  0.3382621762850613 ,acc:  0.9\n",
      "acc: 44\n",
      "epoch:  0 count:  2400 , train loss:  0.47461031425233313 , test loss:  0.3162867258114238 ,acc:  0.88\n",
      "acc: 43\n",
      "epoch:  0 count:  2500 , train loss:  0.4698429604541332 , test loss:  0.3431813940658685 ,acc:  0.86\n",
      "acc: 35\n",
      "epoch:  0 count:  2600 , train loss:  0.47023229660276544 , test loss:  0.4324763342528604 ,acc:  0.7\n",
      "acc: 43\n",
      "epoch:  0 count:  2700 , train loss:  0.468979866295256 , test loss:  0.3374365879977995 ,acc:  0.86\n",
      "acc: 44\n",
      "epoch:  0 count:  2800 , train loss:  0.4665961756851951 , test loss:  0.3407363306709158 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  0 count:  2900 , train loss:  0.46196247344462227 , test loss:  0.3634550915264117 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  3000 , train loss:  0.4583014056389799 , test loss:  0.3726366834934743 ,acc:  0.88\n",
      "acc: 40\n",
      "epoch:  0 count:  3100 , train loss:  0.45393618368222577 , test loss:  0.401504438824486 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  3200 , train loss:  0.4563923305685782 , test loss:  0.34917055488098414 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  3300 , train loss:  0.4554758053636493 , test loss:  0.39902844066396936 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  3400 , train loss:  0.45401996452832116 , test loss:  0.4001426570181502 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  3500 , train loss:  0.45213129035435823 , test loss:  0.40561027415213174 ,acc:  0.86\n",
      "acc: 37\n",
      "epoch:  0 count:  3600 , train loss:  0.45085501473247835 , test loss:  0.4815377917614023 ,acc:  0.74\n",
      "acc: 40\n",
      "epoch:  0 count:  3700 , train loss:  0.4501808621297585 , test loss:  0.41640020685765194 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  3800 , train loss:  0.45107659118936994 , test loss:  0.4051184350502444 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  3900 , train loss:  0.45049267766969586 , test loss:  0.42992109186947347 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  4000 , train loss:  0.4499145040605142 , test loss:  0.3783125217113411 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  4100 , train loss:  0.44717926806474234 , test loss:  0.4034855712274657 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  4200 , train loss:  0.4445481301164509 , test loss:  0.36627607599744805 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  4300 , train loss:  0.44132002853208535 , test loss:  0.4010765789972082 ,acc:  0.82\n",
      "acc: 44\n",
      "epoch:  0 count:  4400 , train loss:  0.4392346627657015 , test loss:  0.3422144282323961 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  0 count:  4500 , train loss:  0.4379044419159628 , test loss:  0.4422337045714994 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  4600 , train loss:  0.43639910398468423 , test loss:  0.37201184747187654 ,acc:  0.88\n",
      "acc: 43\n",
      "epoch:  0 count:  4700 , train loss:  0.4350649811304424 , test loss:  0.37092718450017853 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  4800 , train loss:  0.4345528362845473 , test loss:  0.37089197059656726 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  4900 , train loss:  0.4347637879234198 , test loss:  0.4881738928363484 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  5000 , train loss:  0.4343898254418849 , test loss:  0.4591044295523784 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  5100 , train loss:  0.4338869752421489 , test loss:  0.4209502868171694 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  5200 , train loss:  0.4333041065617263 , test loss:  0.4188607199836406 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  5300 , train loss:  0.43188712751104297 , test loss:  0.3872701295695151 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  5400 , train loss:  0.4318403383712759 , test loss:  0.3497406824871723 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  5500 , train loss:  0.43052030045784356 , test loss:  0.4361267475456566 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  5600 , train loss:  0.43014496150878506 , test loss:  0.46008824930497155 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  5700 , train loss:  0.42984845139042255 , test loss:  0.37682048332386203 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  5800 , train loss:  0.42895405283610594 , test loss:  0.4497459962380161 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  5900 , train loss:  0.42638173796692164 , test loss:  0.4255865191311385 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  6000 , train loss:  0.4266834959671631 , test loss:  0.4238643682513771 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  6100 , train loss:  0.4258008086455045 , test loss:  0.45552536346462147 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  6200 , train loss:  0.4257938454107754 , test loss:  0.4049570492994349 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  6300 , train loss:  0.42331513549576905 , test loss:  0.4320450042129363 ,acc:  0.88\n",
      "acc: 39\n",
      "epoch:  0 count:  6400 , train loss:  0.42325414973354697 , test loss:  0.5100040139291013 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  6500 , train loss:  0.422507053114914 , test loss:  0.40417599235750457 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  6600 , train loss:  0.42198696724818713 , test loss:  0.45059924761510045 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  6700 , train loss:  0.4214469690211624 , test loss:  0.48511766429764464 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  6800 , train loss:  0.4204914875218682 , test loss:  0.42719637889668777 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  0 count:  6900 , train loss:  0.41906740928023917 , test loss:  0.41213912570422506 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  7000 , train loss:  0.41924134127752116 , test loss:  0.4217513159280088 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  7100 , train loss:  0.4188063339384351 , test loss:  0.510372312809759 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  7200 , train loss:  0.41839859586134803 , test loss:  0.45255099780755015 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  7300 , train loss:  0.41913042775533943 , test loss:  0.46875593911056057 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  7400 , train loss:  0.41880600862286493 , test loss:  0.4353657195901269 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  7500 , train loss:  0.4184066656883336 , test loss:  0.41971415509553933 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  7600 , train loss:  0.417135447063028 , test loss:  0.3801277134995462 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  7700 , train loss:  0.4175578860475273 , test loss:  0.407275756332092 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  7800 , train loss:  0.4163078395125441 , test loss:  0.3812083543513654 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  7900 , train loss:  0.4162617317732034 , test loss:  0.4071154419840786 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  8000 , train loss:  0.41463768993405203 , test loss:  0.4727580179613233 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  8100 , train loss:  0.41371627607457023 , test loss:  0.429498806164126 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  8200 , train loss:  0.41239096326302893 , test loss:  0.560076090302509 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  8300 , train loss:  0.41210500866902844 , test loss:  0.4899248352151767 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  8400 , train loss:  0.41106187220666446 , test loss:  0.4418050618976349 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  8500 , train loss:  0.4098616581421122 , test loss:  0.3903583607866629 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  0 count:  8600 , train loss:  0.40871393580187937 , test loss:  0.442885143319163 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  8700 , train loss:  0.40939524657883763 , test loss:  0.48612526292119584 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  8800 , train loss:  0.4093651243656555 , test loss:  0.4224311612031306 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  8900 , train loss:  0.410071150414026 , test loss:  0.49215952222963094 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  9000 , train loss:  0.4101651339505569 , test loss:  0.5487652723595966 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  0 count:  9100 , train loss:  0.4093534755296401 , test loss:  0.43363340526910177 ,acc:  0.86\n",
      "acc: 39\n",
      "epoch:  0 count:  9200 , train loss:  0.4090890936039702 , test loss:  0.6032748986958171 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  0 count:  9300 , train loss:  0.4081550550386299 , test loss:  0.4715520441992703 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  9400 , train loss:  0.40744631469144876 , test loss:  0.4820046999536771 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  9500 , train loss:  0.40607347692277906 , test loss:  0.4733500915172863 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  9600 , train loss:  0.40538380641289634 , test loss:  0.40824377449393523 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  9700 , train loss:  0.4046174001843287 , test loss:  0.5917434383361615 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  9800 , train loss:  0.4053325281567208 , test loss:  0.4124714677054635 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  9900 , train loss:  0.40516120870538924 , test loss:  0.475784130583051 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  10000 , train loss:  0.40471902675913485 , test loss:  0.438433233909991 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  10100 , train loss:  0.405010208850786 , test loss:  0.4229756506378908 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  10200 , train loss:  0.4047317297566659 , test loss:  0.5501115179717363 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  10300 , train loss:  0.40396964703993854 , test loss:  0.5288989459027437 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  10400 , train loss:  0.4043567520046636 , test loss:  0.436911827565782 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  10500 , train loss:  0.4042607683133345 , test loss:  0.4595739631172273 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  10600 , train loss:  0.40463529743476806 , test loss:  0.4540194246322153 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  10700 , train loss:  0.40482328104527154 , test loss:  0.4245568303107757 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  10800 , train loss:  0.40476019479762626 , test loss:  0.5088226089440263 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  10900 , train loss:  0.40405406849246495 , test loss:  0.47747469823193567 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  11000 , train loss:  0.4039982388102382 , test loss:  0.5298636491599598 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  11100 , train loss:  0.4043147601462277 , test loss:  0.6429621502547502 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  11200 , train loss:  0.40493281381875706 , test loss:  0.5288318207708335 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  11300 , train loss:  0.4052817832189944 , test loss:  0.4626511222074623 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  11400 , train loss:  0.40438946487237143 , test loss:  0.4989789030385032 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  11500 , train loss:  0.4041752307991392 , test loss:  0.524803538964452 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  11600 , train loss:  0.4035101255654 , test loss:  0.5758962348449097 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  11700 , train loss:  0.4027903496983052 , test loss:  0.5081061485589783 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  11800 , train loss:  0.4017364829861191 , test loss:  0.6331071982967541 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  11900 , train loss:  0.4027348697127057 , test loss:  0.5118102040001941 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  12000 , train loss:  0.40264477805677273 , test loss:  0.5528006089380504 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  12100 , train loss:  0.40221697515138094 , test loss:  0.45299206075416804 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  12200 , train loss:  0.4021412123237104 , test loss:  0.5320853153074853 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  12300 , train loss:  0.4018577199677743 , test loss:  0.5074194414264093 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  12400 , train loss:  0.40149657561882107 , test loss:  0.4946549412136665 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  12500 , train loss:  0.40060926458533547 , test loss:  0.6011202081275633 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  12600 , train loss:  0.40152342520792184 , test loss:  0.428771007106975 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  12700 , train loss:  0.4007054639423277 , test loss:  0.5430476545229601 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  12800 , train loss:  0.39944007429150113 , test loss:  0.6171728207760839 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  12900 , train loss:  0.4001563004101115 , test loss:  0.4330917841195787 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  13000 , train loss:  0.39984640249814396 , test loss:  0.5154456127466982 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  13100 , train loss:  0.3998380296235533 , test loss:  0.44702684306164886 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  13200 , train loss:  0.3991316056832263 , test loss:  0.5463811397983226 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  13300 , train loss:  0.3989428558053312 , test loss:  0.49959716182414465 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  13400 , train loss:  0.3991722909463688 , test loss:  0.5215866750816347 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  13500 , train loss:  0.3983151494533262 , test loss:  0.526386502436744 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  13600 , train loss:  0.39780637215912773 , test loss:  0.5296121997832302 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  13700 , train loss:  0.39744303759511945 , test loss:  0.5408200296050165 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  13800 , train loss:  0.39739564899490504 , test loss:  0.47141797558258985 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  13900 , train loss:  0.39697691229863474 , test loss:  0.46835188476743156 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  14000 , train loss:  0.3970271493302683 , test loss:  0.5136954021881683 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  14100 , train loss:  0.39648182267843524 , test loss:  0.5260448718655971 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  14200 , train loss:  0.39559220593855643 , test loss:  0.6295255124765459 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  14300 , train loss:  0.3959311545209287 , test loss:  0.5573213955556264 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  14400 , train loss:  0.39535778487031537 , test loss:  0.5545822403895636 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  14500 , train loss:  0.39522690446661146 , test loss:  0.5524514217586329 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  14600 , train loss:  0.39449396476924903 , test loss:  0.6136777081873492 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  14700 , train loss:  0.39427146855521783 , test loss:  0.7334720750584984 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  14800 , train loss:  0.39357526863784686 , test loss:  0.730377470040463 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  14900 , train loss:  0.39410750676767076 , test loss:  0.663805262204114 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  15000 , train loss:  0.39430932475090097 , test loss:  0.5165736646021105 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  15100 , train loss:  0.39358393884795667 , test loss:  0.6607277544841327 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  15200 , train loss:  0.3934689633938784 , test loss:  0.5586075824850013 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  15300 , train loss:  0.39349335881746705 , test loss:  0.473844836646349 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  15400 , train loss:  0.39320640786783256 , test loss:  0.5152141966817635 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  15500 , train loss:  0.3928221365583877 , test loss:  0.5264111504116635 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  15600 , train loss:  0.39285157220209876 , test loss:  0.5324156818661868 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  15700 , train loss:  0.39300299433621017 , test loss:  0.48487395276459877 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  15800 , train loss:  0.39322754461216286 , test loss:  0.44874909519072387 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  15900 , train loss:  0.39315574453586905 , test loss:  0.4779376387416414 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  16000 , train loss:  0.3930566144448756 , test loss:  0.4481083790840057 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  16100 , train loss:  0.39319569793469145 , test loss:  0.47697338338938605 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  16200 , train loss:  0.39337271348679664 , test loss:  0.5244614909880329 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  16300 , train loss:  0.3934736007603427 , test loss:  0.4811305519012967 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  16400 , train loss:  0.3930343174356093 , test loss:  0.4625766645409169 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  16500 , train loss:  0.39276694012605906 , test loss:  0.4867788570462278 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  16600 , train loss:  0.39212406854339793 , test loss:  0.41964424583901744 ,acc:  0.82\n",
      "acc: 37\n",
      "epoch:  0 count:  16700 , train loss:  0.3922497876192585 , test loss:  0.6455349931100488 ,acc:  0.74\n",
      "acc: 38\n",
      "epoch:  0 count:  16800 , train loss:  0.39171101090016397 , test loss:  0.6319723218621539 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  16900 , train loss:  0.39192682815076324 , test loss:  0.44599284112612625 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  17000 , train loss:  0.39145127010925557 , test loss:  0.5518279312524554 ,acc:  0.82\n",
      "acc: 34\n",
      "epoch:  0 count:  17100 , train loss:  0.39106566109513097 , test loss:  0.7584645878121318 ,acc:  0.68\n",
      "acc: 40\n",
      "epoch:  0 count:  17200 , train loss:  0.39107962572357463 , test loss:  0.542650862898222 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  17300 , train loss:  0.3911115308844347 , test loss:  0.4442401849021371 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  17400 , train loss:  0.39085067987027383 , test loss:  0.4669881771539713 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  17500 , train loss:  0.39051264231418037 , test loss:  0.46285410167590046 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  17600 , train loss:  0.39023533904652974 , test loss:  0.5075401426225926 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  17700 , train loss:  0.3899802586406223 , test loss:  0.4882213146797949 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  17800 , train loss:  0.3899772091019512 , test loss:  0.5672121681180943 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  17900 , train loss:  0.38999804155918966 , test loss:  0.4289557105503626 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  18000 , train loss:  0.3889077661503871 , test loss:  0.4948162134859318 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  18100 , train loss:  0.38876931987846036 , test loss:  0.5396238619850737 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  18200 , train loss:  0.3883478569496424 , test loss:  0.5412791092755321 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  18300 , train loss:  0.38863028968846497 , test loss:  0.5227957521563803 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  18400 , train loss:  0.3886578598819453 , test loss:  0.5813909690637593 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  18500 , train loss:  0.38911615248500453 , test loss:  0.5737920098472387 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  18600 , train loss:  0.38887567534158013 , test loss:  0.5469125579531101 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  18700 , train loss:  0.3887251870844427 , test loss:  0.43975365067250094 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  18800 , train loss:  0.38829193281819074 , test loss:  0.43507178250129075 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  18900 , train loss:  0.38836152909274435 , test loss:  0.5936822296165701 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  19000 , train loss:  0.3883041433579514 , test loss:  0.47565300134854627 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  19100 , train loss:  0.3881392299349539 , test loss:  0.520071295446869 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  19200 , train loss:  0.3871547460738597 , test loss:  0.5532703362488877 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  19300 , train loss:  0.3866084833136022 , test loss:  0.668229783286406 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  19400 , train loss:  0.38627614876025007 , test loss:  0.6580064931016 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  19500 , train loss:  0.38641017898745883 , test loss:  0.5217379177812631 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  19600 , train loss:  0.3859122568255171 , test loss:  0.5853633015809896 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  19700 , train loss:  0.38570941716335644 , test loss:  0.5251355827205407 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  19800 , train loss:  0.3857655798769136 , test loss:  0.6299235978367597 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  19900 , train loss:  0.3853499958701225 , test loss:  0.6297469538072664 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  20000 , train loss:  0.3848131258264016 , test loss:  0.5986690110768994 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  20100 , train loss:  0.3854487634462497 , test loss:  0.6496927859226708 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  0 count:  20200 , train loss:  0.3861901716062345 , test loss:  0.40615750405679135 ,acc:  0.86\n",
      "acc: 35\n",
      "epoch:  0 count:  20300 , train loss:  0.38594463561487596 , test loss:  0.6820720015295956 ,acc:  0.7\n",
      "acc: 35\n",
      "epoch:  0 count:  20400 , train loss:  0.385913829001499 , test loss:  0.629242871019261 ,acc:  0.7\n",
      "acc: 38\n",
      "epoch:  0 count:  20500 , train loss:  0.3858601972742588 , test loss:  0.594162446882674 ,acc:  0.76\n",
      "acc: 42\n",
      "epoch:  0 count:  20600 , train loss:  0.38557952545749075 , test loss:  0.5191829666843705 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  20700 , train loss:  0.38514974108171546 , test loss:  0.5201158182787183 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  20800 , train loss:  0.38443776907430893 , test loss:  0.5165934690013728 ,acc:  0.84\n",
      "acc: 38\n",
      "epoch:  0 count:  20900 , train loss:  0.3846899674519221 , test loss:  0.5678633455595559 ,acc:  0.76\n",
      "acc: 42\n",
      "epoch:  0 count:  21000 , train loss:  0.38427898890576123 , test loss:  0.5076790619214806 ,acc:  0.84\n",
      "acc: 38\n",
      "epoch:  0 count:  21100 , train loss:  0.38441854883440785 , test loss:  0.45276407152318143 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  21200 , train loss:  0.3844961000492489 , test loss:  0.489166232829848 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  21300 , train loss:  0.3845950479738068 , test loss:  0.4889486670319957 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  21400 , train loss:  0.3843632265074985 , test loss:  0.4434656574245014 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  21500 , train loss:  0.38443689037802264 , test loss:  0.533971504783749 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  21600 , train loss:  0.3839161491820778 , test loss:  0.6068908410740369 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  21700 , train loss:  0.38364995802737106 , test loss:  0.44055636288824757 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  21800 , train loss:  0.38333778039338506 , test loss:  0.4851906252808014 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  21900 , train loss:  0.38310468032875006 , test loss:  0.4155641896761415 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  22000 , train loss:  0.3825333901294635 , test loss:  0.49955423126855053 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  22100 , train loss:  0.3825288934714865 , test loss:  0.42772693083790725 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  22200 , train loss:  0.3828486359070134 , test loss:  0.46455548505236893 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  22300 , train loss:  0.3827040461968131 , test loss:  0.44392428826422475 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  22400 , train loss:  0.3822871879113504 , test loss:  0.5562294789993387 ,acc:  0.82\n",
      "acc: 44\n",
      "epoch:  0 count:  22500 , train loss:  0.3823385963888249 , test loss:  0.4774996883530332 ,acc:  0.88\n",
      "acc: 41\n",
      "epoch:  0 count:  22600 , train loss:  0.38256978126856656 , test loss:  0.49250213174940655 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  22700 , train loss:  0.3824441064693446 , test loss:  0.4908529781203718 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  22800 , train loss:  0.382737719452133 , test loss:  0.48480474576299915 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  0 count:  22900 , train loss:  0.3823941070122298 , test loss:  0.5124629658726804 ,acc:  0.86\n",
      "acc: 39\n",
      "epoch:  0 count:  23000 , train loss:  0.38260607894395093 , test loss:  0.6362140396508948 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  23100 , train loss:  0.38215628220303605 , test loss:  0.5785279319855345 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  23200 , train loss:  0.38189150992392223 , test loss:  0.5392263897851626 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  23300 , train loss:  0.38175221229133677 , test loss:  0.6429610417918138 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  23400 , train loss:  0.38173096165440806 , test loss:  0.565083103393772 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  23500 , train loss:  0.38166862133072976 , test loss:  0.6419077109832597 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  23600 , train loss:  0.38160432396704136 , test loss:  0.8753147511711902 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  23700 , train loss:  0.38140059203208765 , test loss:  0.678383342785811 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  23800 , train loss:  0.3813162692863372 , test loss:  0.658161021797348 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  23900 , train loss:  0.38156164197777087 , test loss:  0.6184036226602256 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  24000 , train loss:  0.3820139628251037 , test loss:  0.7185282979276326 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  24100 , train loss:  0.3819922749755269 , test loss:  0.5923768686888615 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  24200 , train loss:  0.3821332486448135 , test loss:  0.5487909164468122 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  24300 , train loss:  0.3819208198499592 , test loss:  0.7156066137683229 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  24400 , train loss:  0.3819458856314562 , test loss:  0.6890116572876995 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  24500 , train loss:  0.3816989230905731 , test loss:  0.6417833892049043 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  24600 , train loss:  0.38125771217649446 , test loss:  0.765564613613874 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  24700 , train loss:  0.38098608863111594 , test loss:  0.6299505905767592 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  24800 , train loss:  0.38049586384550116 , test loss:  0.5558723357981448 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  24900 , train loss:  0.38071706585330056 , test loss:  0.524065609262093 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  25000 , train loss:  0.3803754422324832 , test loss:  0.6195123741554756 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  25100 , train loss:  0.3804606437563445 , test loss:  0.5268885074367426 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  25200 , train loss:  0.3807493998887189 , test loss:  0.46817313610983546 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  25300 , train loss:  0.380638052092747 , test loss:  0.5075671326414977 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  25400 , train loss:  0.3806835666362919 , test loss:  0.5355814428425038 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  25500 , train loss:  0.3807335233311969 , test loss:  0.5176101249406565 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  25600 , train loss:  0.3805457125479946 , test loss:  0.5850430903046708 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  25700 , train loss:  0.38044357277262314 , test loss:  0.5839282828584532 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  25800 , train loss:  0.3805596507691843 , test loss:  0.6442898497810166 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  25900 , train loss:  0.3806136646168889 , test loss:  0.6156328895618753 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  26000 , train loss:  0.38034931442246916 , test loss:  0.6775188111710655 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  26100 , train loss:  0.38026726143052586 , test loss:  0.7455197801873453 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  26200 , train loss:  0.3804655394683984 , test loss:  0.8464768203829669 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  26300 , train loss:  0.38049186106769006 , test loss:  0.7164445653782963 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  26400 , train loss:  0.3804640886015125 , test loss:  0.5814836919070331 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  26500 , train loss:  0.3803658064458449 , test loss:  0.5568064307261147 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  26600 , train loss:  0.38006012071103507 , test loss:  0.5824619347904423 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  26700 , train loss:  0.38017820412760944 , test loss:  0.5525784242148165 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  26800 , train loss:  0.3800909044990255 , test loss:  0.7148089984329203 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  26900 , train loss:  0.37990601726615936 , test loss:  0.6359284484387102 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  27000 , train loss:  0.37976918972935425 , test loss:  0.7368895639107405 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  27100 , train loss:  0.3794125342683761 , test loss:  0.6500821657748399 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  27200 , train loss:  0.37937630361906954 , test loss:  0.7083703462844368 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  27300 , train loss:  0.3790793251157052 , test loss:  0.5516965755445458 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  27400 , train loss:  0.37911490865637004 , test loss:  0.5454508833820535 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  27500 , train loss:  0.37913635862450096 , test loss:  0.5650279135145411 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  27600 , train loss:  0.3787798730138598 , test loss:  0.6504791729852916 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  27700 , train loss:  0.37859764496860737 , test loss:  0.6381885276731003 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  27800 , train loss:  0.37846318009342567 , test loss:  0.7193630831984501 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  27900 , train loss:  0.3782363763485047 , test loss:  0.6300911471483505 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  28000 , train loss:  0.37806503328263014 , test loss:  0.6269105544112424 ,acc:  0.84\n",
      "acc: 38\n",
      "epoch:  0 count:  28100 , train loss:  0.37790568485210774 , test loss:  0.8578838874427741 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  28200 , train loss:  0.37775899199353047 , test loss:  0.7067933321944616 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  28300 , train loss:  0.37760828791270934 , test loss:  0.6899441867394829 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  28400 , train loss:  0.3776209525920007 , test loss:  0.5679492499247748 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  28500 , train loss:  0.3774131709883853 , test loss:  0.7075641055017786 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  28600 , train loss:  0.377378944990845 , test loss:  0.5494578651620406 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  28700 , train loss:  0.3770007940747652 , test loss:  0.6557818052145893 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  28800 , train loss:  0.37724018373166474 , test loss:  0.5694063322081974 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  28900 , train loss:  0.37717966868112524 , test loss:  0.77277473930604 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  29000 , train loss:  0.3774266408763521 , test loss:  0.6079996231329414 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  29100 , train loss:  0.37726321787122286 , test loss:  0.6052721800347718 ,acc:  0.86\n",
      "acc: 39\n",
      "epoch:  0 count:  29200 , train loss:  0.3773728371954006 , test loss:  0.6797926191351837 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  29300 , train loss:  0.3774390939838127 , test loss:  0.5693143073300871 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  29400 , train loss:  0.377251421209201 , test loss:  0.5973537988943616 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  29500 , train loss:  0.37742484668788817 , test loss:  0.5907214261830995 ,acc:  0.86\n",
      "acc: 37\n",
      "epoch:  0 count:  29600 , train loss:  0.3770430773122669 , test loss:  0.9345636023828632 ,acc:  0.74\n",
      "acc: 42\n",
      "epoch:  0 count:  29700 , train loss:  0.37726234403890396 , test loss:  0.5203603286528596 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  29800 , train loss:  0.3771897862481563 , test loss:  0.7928508926088124 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  29900 , train loss:  0.3774635897886182 , test loss:  0.7145317227661099 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  30000 , train loss:  0.37760806962687676 , test loss:  0.4936905496345753 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  30100 , train loss:  0.3774439259580463 , test loss:  0.6857832113991867 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  30200 , train loss:  0.3769692646956619 , test loss:  0.9688366738254532 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  30300 , train loss:  0.37664303623080286 , test loss:  0.877565657713385 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  30400 , train loss:  0.3767414191978752 , test loss:  0.7725242374161193 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  30500 , train loss:  0.376534409596767 , test loss:  0.688228020236781 ,acc:  0.78\n",
      "acc: 38\n",
      "epoch:  0 count:  30600 , train loss:  0.376722435887938 , test loss:  0.7140194868249817 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  30700 , train loss:  0.37666527572728253 , test loss:  0.6931526132850536 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  30800 , train loss:  0.37624611820373044 , test loss:  0.7768575301707914 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  30900 , train loss:  0.3762425564827956 , test loss:  0.6613953796297938 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  31000 , train loss:  0.376295171243932 , test loss:  0.6231497316625427 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  31100 , train loss:  0.3763758355912895 , test loss:  0.5952029441567902 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  31200 , train loss:  0.37613084663013596 , test loss:  0.9727677855721316 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  31300 , train loss:  0.3761609441984534 , test loss:  0.8049485705086739 ,acc:  0.8\n",
      "acc: 37\n",
      "epoch:  0 count:  31400 , train loss:  0.37616763109683093 , test loss:  0.8686220625858232 ,acc:  0.74\n",
      "acc: 39\n",
      "epoch:  0 count:  31500 , train loss:  0.3761367764183363 , test loss:  0.8165712141790549 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  31600 , train loss:  0.37639253608693335 , test loss:  0.7476643304223028 ,acc:  0.78\n",
      "acc: 38\n",
      "epoch:  0 count:  31700 , train loss:  0.37646947290417604 , test loss:  0.7633911374703665 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  31800 , train loss:  0.37634671954396576 , test loss:  0.8521857397676217 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  31900 , train loss:  0.3760708555350965 , test loss:  0.8795831493360265 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  32000 , train loss:  0.37633002029445406 , test loss:  0.7935299263139939 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  32100 , train loss:  0.37638438831662857 , test loss:  0.8083143326841218 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  32200 , train loss:  0.37640927095929694 , test loss:  0.8523795113315711 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  32300 , train loss:  0.37668953331482724 , test loss:  0.9206934282654038 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  32400 , train loss:  0.37639789510394117 , test loss:  0.7826630641183079 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  32500 , train loss:  0.37603718279351334 , test loss:  0.8310142541441595 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  32600 , train loss:  0.37587918766717987 , test loss:  0.9633473817115328 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  32700 , train loss:  0.3754929219858019 , test loss:  0.9919754923078267 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  32800 , train loss:  0.37547833342222653 , test loss:  0.6511354611231213 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  32900 , train loss:  0.37562561144272105 , test loss:  0.7805667837907024 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  33000 , train loss:  0.37570650548712464 , test loss:  0.7065116007329185 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  33100 , train loss:  0.37543530009552856 , test loss:  0.678755715874845 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  33200 , train loss:  0.3756651560394498 , test loss:  0.5884801078900546 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  33300 , train loss:  0.3754799746310476 , test loss:  0.8106440779691106 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  33400 , train loss:  0.37570522325185807 , test loss:  0.8896477432422373 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  33500 , train loss:  0.3758445871513621 , test loss:  0.6318710429296425 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  33600 , train loss:  0.3757563174908007 , test loss:  0.6760047998123264 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  33700 , train loss:  0.3758368063145916 , test loss:  0.7039330444580537 ,acc:  0.76\n",
      "acc: 38\n",
      "epoch:  0 count:  33800 , train loss:  0.3758101398127803 , test loss:  0.8119251378541026 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  33900 , train loss:  0.37556730227613194 , test loss:  0.7977995380581091 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  34000 , train loss:  0.37574273571993844 , test loss:  0.5581122451149713 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  34100 , train loss:  0.3761271153832497 , test loss:  0.5661998805094846 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  34200 , train loss:  0.3757302813004743 , test loss:  0.7241092572746878 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  34300 , train loss:  0.37563946732491565 , test loss:  0.6445802061422501 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  34400 , train loss:  0.3756053098664875 , test loss:  0.8627747539621395 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  34500 , train loss:  0.37507682468737896 , test loss:  1.1510055938801316 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  34600 , train loss:  0.3749489929745597 , test loss:  0.5235706417990332 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  34700 , train loss:  0.3749019814711741 , test loss:  0.6512047568065656 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  34800 , train loss:  0.37478939412706747 , test loss:  1.0543104862165626 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  34900 , train loss:  0.3748671353145479 , test loss:  0.5881771593742086 ,acc:  0.78\n",
      "acc: 38\n",
      "epoch:  0 count:  35000 , train loss:  0.3750033725527832 , test loss:  0.6035541624311008 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  35100 , train loss:  0.37478026288943395 , test loss:  0.5624554456196518 ,acc:  0.78\n",
      "acc: 38\n",
      "epoch:  0 count:  35200 , train loss:  0.374799772043256 , test loss:  0.7980913951199227 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  35300 , train loss:  0.3748973973712324 , test loss:  0.6847191002637646 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  35400 , train loss:  0.3746519230814321 , test loss:  0.6948191952139302 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  35500 , train loss:  0.3745770874047989 , test loss:  0.7823025694513808 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  35600 , train loss:  0.3745193128683477 , test loss:  0.7209373859621939 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  35700 , train loss:  0.3744968273997566 , test loss:  0.6752568433573742 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  35800 , train loss:  0.374454179407166 , test loss:  0.48509488577412413 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  35900 , train loss:  0.37448447396368106 , test loss:  0.500716678809622 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  36000 , train loss:  0.3743356091426191 , test loss:  0.6009881705573842 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  36100 , train loss:  0.37443035823721227 , test loss:  0.6568751921967759 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  36200 , train loss:  0.374720227531337 , test loss:  0.41950265338915416 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  36300 , train loss:  0.3745751245614462 , test loss:  0.601500469396413 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  36400 , train loss:  0.37432255739715653 , test loss:  0.643487748083525 ,acc:  0.78\n",
      "acc: 38\n",
      "epoch:  0 count:  36500 , train loss:  0.3745575657224733 , test loss:  0.6134169953061882 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  36600 , train loss:  0.37452774312147663 , test loss:  0.7219106804541048 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  36700 , train loss:  0.3744140634902553 , test loss:  0.6766994460647123 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  36800 , train loss:  0.3740710900322703 , test loss:  0.5122016110196358 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  36900 , train loss:  0.37395186683910914 , test loss:  0.81090202845146 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  37000 , train loss:  0.3739253014182112 , test loss:  0.49418037910500456 ,acc:  0.86\n",
      "acc: 42\n",
      "epoch:  0 count:  37100 , train loss:  0.37377524796216416 , test loss:  0.5332142239023402 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  37200 , train loss:  0.373519363649841 , test loss:  0.6089793690979558 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  37300 , train loss:  0.3735248913574848 , test loss:  0.5697307098009877 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  37400 , train loss:  0.37323467171209995 , test loss:  0.588776220161959 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  37500 , train loss:  0.3731312266513084 , test loss:  0.5994867236814474 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  37600 , train loss:  0.37308444115777706 , test loss:  0.6605321815369598 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  37700 , train loss:  0.3731197555917424 , test loss:  0.4938529076363373 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  37800 , train loss:  0.3735525079638792 , test loss:  0.5183929862966761 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  37900 , train loss:  0.3735698398291912 , test loss:  0.6135413415775559 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  38000 , train loss:  0.3734711427585536 , test loss:  0.5673262184910687 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  38100 , train loss:  0.3735984379342472 , test loss:  0.6211255689862446 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  38200 , train loss:  0.373491797369577 , test loss:  0.6958743443175714 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  38300 , train loss:  0.37348337333381504 , test loss:  0.47444870217632285 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  38400 , train loss:  0.3734925756187704 , test loss:  0.609720338950699 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  38500 , train loss:  0.37341368974743433 , test loss:  0.5521621474242738 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  38600 , train loss:  0.37357651930247576 , test loss:  0.7046639030320057 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  38700 , train loss:  0.3735395067956882 , test loss:  0.5432648514959145 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  38800 , train loss:  0.3734422561224977 , test loss:  0.6077915479164175 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  38900 , train loss:  0.3735296770480249 , test loss:  0.5809835295335506 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  39000 , train loss:  0.3733903931320777 , test loss:  0.5646071937667876 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  39100 , train loss:  0.3732031521953342 , test loss:  0.68730805500496 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  39200 , train loss:  0.37323617247347013 , test loss:  0.6883673817849967 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  39300 , train loss:  0.3731540307916986 , test loss:  0.43773393591266996 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  39400 , train loss:  0.37332561917757995 , test loss:  0.5275467979526729 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  39500 , train loss:  0.3732893871804405 , test loss:  0.516424101954235 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  39600 , train loss:  0.37336708843750605 , test loss:  0.5401827921350195 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  39700 , train loss:  0.37311174496991695 , test loss:  0.7456815782271314 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  39800 , train loss:  0.3728281176890504 , test loss:  0.6951280553519075 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  39900 , train loss:  0.37267588532438517 , test loss:  0.6876681659363021 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  40000 , train loss:  0.37264838338876 , test loss:  0.5991919259241688 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  40100 , train loss:  0.37267927922153254 , test loss:  0.4420842036636657 ,acc:  0.84\n",
      "acc: 43\n",
      "epoch:  0 count:  40200 , train loss:  0.37255820424755787 , test loss:  0.5676839815356947 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  40300 , train loss:  0.3725752841090738 , test loss:  0.6022458715221455 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  40400 , train loss:  0.37266326485623796 , test loss:  0.6151185827257586 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  40500 , train loss:  0.3725007887542006 , test loss:  0.553830229869518 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  40600 , train loss:  0.3724021029094222 , test loss:  0.8024319415915647 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  40700 , train loss:  0.37241168914128275 , test loss:  0.552519724811582 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  40800 , train loss:  0.37220654856566454 , test loss:  0.5916412447478921 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  40900 , train loss:  0.3722913471676007 , test loss:  0.5944249875243236 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  41000 , train loss:  0.372333913174029 , test loss:  0.6147404272491189 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  41100 , train loss:  0.3721434258891424 , test loss:  0.7407275368276879 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  41200 , train loss:  0.37221631842425823 , test loss:  0.6689721639979144 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  0 count:  41300 , train loss:  0.37227231689351864 , test loss:  0.6290027920415162 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  0 count:  41400 , train loss:  0.37205419567157816 , test loss:  0.7695574483239578 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  41500 , train loss:  0.37207192051643584 , test loss:  0.7798068219916966 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  41600 , train loss:  0.37203223546971115 , test loss:  0.8728763799536603 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  41700 , train loss:  0.3719057200382594 , test loss:  0.8114932223664221 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  41800 , train loss:  0.3718532525882208 , test loss:  0.7361349985770176 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  41900 , train loss:  0.37159546096327833 , test loss:  0.9213316410277002 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  42000 , train loss:  0.3714593966207517 , test loss:  0.936156740348565 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  42100 , train loss:  0.37176540411244924 , test loss:  0.7646259328199076 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  42200 , train loss:  0.3719119980005145 , test loss:  0.5128891714306701 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  42300 , train loss:  0.371802175397556 , test loss:  0.6126305274266906 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  42400 , train loss:  0.37170094816284205 , test loss:  0.7144802246409334 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  42500 , train loss:  0.37164294901090267 , test loss:  0.8197199202106638 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  42600 , train loss:  0.3716996015348141 , test loss:  0.7036840599980927 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  42700 , train loss:  0.3717347560324382 , test loss:  0.7319154986889578 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  42800 , train loss:  0.3715632646892142 , test loss:  0.6398042287200199 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  42900 , train loss:  0.37128913027799715 , test loss:  0.6479930279386689 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  0 count:  43000 , train loss:  0.37150722295834404 , test loss:  0.6427738554571243 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  0 count:  43100 , train loss:  0.371677612230446 , test loss:  0.8287806920484934 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  0 count:  43200 , train loss:  0.3717579894426433 , test loss:  0.7563537005159731 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  43300 , train loss:  0.3716061201629574 , test loss:  0.6145963295345102 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  43400 , train loss:  0.37154474592782666 , test loss:  0.633779413565037 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  0 count:  43500 , train loss:  0.3713743270287067 , test loss:  0.9559816577717595 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  0 count:  43600 , train loss:  0.37146684071797936 , test loss:  0.5284630697179602 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  43700 , train loss:  0.3711885888394083 , test loss:  0.6171000400938099 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  43800 , train loss:  0.3713040878369332 , test loss:  0.627438548979971 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  43900 , train loss:  0.3713612516673174 , test loss:  0.7168104885377714 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  44000 , train loss:  0.3711513265316068 , test loss:  0.669700940870124 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  44100 , train loss:  0.37090358539162965 , test loss:  0.7764457887467922 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  44200 , train loss:  0.37113213754787955 , test loss:  0.5571371332521806 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  44300 , train loss:  0.3709422468764478 , test loss:  0.5940635444652526 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  0 count:  44400 , train loss:  0.3710679464687053 , test loss:  0.5687519280140987 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  44500 , train loss:  0.37096442566404064 , test loss:  0.6398452766439664 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  44600 , train loss:  0.3708520314558015 , test loss:  0.6404274058659751 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  44700 , train loss:  0.37094738738422395 , test loss:  0.6097640739852068 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  44800 , train loss:  0.371132592120548 , test loss:  0.5585625652121143 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  44900 , train loss:  0.3711337329696642 , test loss:  0.6329037208716863 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  45000 , train loss:  0.3709528787797617 , test loss:  0.6516577637909051 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  45100 , train loss:  0.3709851739594026 , test loss:  0.8700684972502859 ,acc:  0.76\n",
      "acc: 41\n",
      "epoch:  0 count:  45200 , train loss:  0.37081014328040807 , test loss:  0.6852383605005421 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  45300 , train loss:  0.37072025100220135 , test loss:  0.8200063707637452 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  45400 , train loss:  0.37077372480595044 , test loss:  0.5719357758079423 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  45500 , train loss:  0.3706955472579393 , test loss:  0.6003890453304211 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  45600 , train loss:  0.37075967521580677 , test loss:  0.5228122668940568 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  45700 , train loss:  0.37076304657061604 , test loss:  0.6180028176368887 ,acc:  0.8\n",
      "acc: 37\n",
      "epoch:  0 count:  45800 , train loss:  0.37085101324790304 , test loss:  0.7265521013279761 ,acc:  0.74\n",
      "acc: 39\n",
      "epoch:  0 count:  45900 , train loss:  0.370855905009633 , test loss:  0.8429080162190821 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  46000 , train loss:  0.370779684595624 , test loss:  0.6322592762658718 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  46100 , train loss:  0.37079473533057156 , test loss:  0.5840302555629296 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  46200 , train loss:  0.3707966520470337 , test loss:  0.5296426348703789 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  46300 , train loss:  0.3707153197931676 , test loss:  0.6050535357194613 ,acc:  0.82\n",
      "acc: 44\n",
      "epoch:  0 count:  46400 , train loss:  0.37082474284094524 , test loss:  0.5974146422529145 ,acc:  0.88\n",
      "acc: 41\n",
      "epoch:  0 count:  46500 , train loss:  0.37103000519004814 , test loss:  0.6623735148686773 ,acc:  0.82\n",
      "acc: 38\n",
      "epoch:  0 count:  46600 , train loss:  0.371223552433397 , test loss:  0.760034966814481 ,acc:  0.76\n",
      "acc: 40\n",
      "epoch:  0 count:  46700 , train loss:  0.37119190310534295 , test loss:  0.7125240485759786 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  0 count:  46800 , train loss:  0.3709072474733922 , test loss:  0.6795860694267492 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  0 count:  46900 , train loss:  0.37086292710800717 , test loss:  0.7376024128393189 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  0 count:  47000 , train loss:  0.37070369197429615 , test loss:  0.7201998014784653 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  0 count:  47100 , train loss:  0.37056027941013836 , test loss:  0.7568864345202019 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  0 count:  47200 , train loss:  0.3706387805077699 , test loss:  0.5609449864260933 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  0 count:  47300 , train loss:  0.37080997781808556 , test loss:  0.58935968903711 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  0 count:  47400 , train loss:  0.3709413497308061 , test loss:  0.6844200116673866 ,acc:  0.8\n",
      "acc: 44\n",
      "epoch:  0 count:  47500 , train loss:  0.3707603742444282 , test loss:  0.6114109120199339 ,acc:  0.88\n",
      "acc: 39\n",
      "epoch:  0 count:  47600 , train loss:  0.37067440127911744 , test loss:  0.8316263242305013 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  47700 , train loss:  0.37085074666356466 , test loss:  0.5705468557770602 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  0 count:  47800 , train loss:  0.37071760213866833 , test loss:  0.7179718603562367 ,acc:  0.88\n",
      "acc: 40\n",
      "epoch:  0 count:  47900 , train loss:  0.3705175864866736 , test loss:  0.8725240349633526 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  0 count:  48000 , train loss:  0.3703700929987088 , test loss:  0.9604058347400951 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  0 count:  48100 , train loss:  0.3701355055314971 , test loss:  0.8399977590247909 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  0 count:  48200 , train loss:  0.369974136252077 , test loss:  1.0772137332955025 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  0 count:  48300 , train loss:  0.36975847668050754 , test loss:  0.9360299889353172 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  0 count:  48400 , train loss:  0.3696212718220036 , test loss:  0.738975737722211 ,acc:  0.78\n",
      "acc: 42\n",
      "epoch:  0 count:  48500 , train loss:  0.36947684715488405 , test loss:  0.7442791429724457 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  0 count:  48600 , train loss:  0.369197152954727 , test loss:  0.9501947998230116 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  0 count:  48700 , train loss:  0.3693315114788244 , test loss:  0.8115839811967931 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  100 , train loss:  0.40613314167894715 , test loss:  0.6531900010432206 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  1 count:  200 , train loss:  0.41982034094144866 , test loss:  0.5400245049530343 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  1 count:  300 , train loss:  0.39403276669391857 , test loss:  0.5678771302603018 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  1 count:  400 , train loss:  0.3813140115767906 , test loss:  0.7625143758862726 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  1 count:  500 , train loss:  0.371321223389515 , test loss:  0.635236399920592 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  600 , train loss:  0.3646660220196736 , test loss:  0.6826427970480176 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  1 count:  700 , train loss:  0.35786279182451 , test loss:  0.5278810885138426 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  1 count:  800 , train loss:  0.35391715166107035 , test loss:  0.7306234510309559 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  1 count:  900 , train loss:  0.363970520517655 , test loss:  0.7670654823520651 ,acc:  0.8\n",
      "acc: 37\n",
      "epoch:  1 count:  1000 , train loss:  0.3643651149283468 , test loss:  0.8230871286119599 ,acc:  0.74\n",
      "acc: 39\n",
      "epoch:  1 count:  1100 , train loss:  0.36429200863380146 , test loss:  0.826885111851153 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  1200 , train loss:  0.3685174067143482 , test loss:  0.5980101388574723 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  1 count:  1300 , train loss:  0.36769633925647616 , test loss:  0.8000476177647157 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  1400 , train loss:  0.3822791062432481 , test loss:  0.6831271688677362 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  1500 , train loss:  0.38773211545691993 , test loss:  0.7409349836600811 ,acc:  0.8\n",
      "acc: 38\n",
      "epoch:  1 count:  1600 , train loss:  0.3851357962585881 , test loss:  0.5938616961393655 ,acc:  0.76\n",
      "acc: 39\n",
      "epoch:  1 count:  1700 , train loss:  0.3831611389505377 , test loss:  0.8387734105847966 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  1800 , train loss:  0.3785844747980773 , test loss:  0.8990767666350229 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  1900 , train loss:  0.37816869269899456 , test loss:  0.7600628616754528 ,acc:  0.78\n",
      "acc: 36\n",
      "epoch:  1 count:  2000 , train loss:  0.376867880083368 , test loss:  0.8712928800464397 ,acc:  0.72\n",
      "acc: 43\n",
      "epoch:  1 count:  2100 , train loss:  0.3791930070256978 , test loss:  0.5280075589263936 ,acc:  0.86\n",
      "acc: 39\n",
      "epoch:  1 count:  2200 , train loss:  0.3796674165674115 , test loss:  0.6988200779862757 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  2300 , train loss:  0.378420716852677 , test loss:  0.6045125575465409 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  1 count:  2400 , train loss:  0.3767674987999808 , test loss:  0.45814171776016793 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  1 count:  2500 , train loss:  0.37162566915416345 , test loss:  0.5783761669144303 ,acc:  0.8\n",
      "acc: 40\n",
      "epoch:  1 count:  2600 , train loss:  0.37508450712240293 , test loss:  0.6577964011019497 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  1 count:  2700 , train loss:  0.37709093662582777 , test loss:  0.5748948447778446 ,acc:  0.86\n",
      "acc: 39\n",
      "epoch:  1 count:  2800 , train loss:  0.3764517637779646 , test loss:  0.7806906491324571 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  1 count:  2900 , train loss:  0.3750483799496452 , test loss:  0.675217824975083 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  1 count:  3000 , train loss:  0.3735416205312005 , test loss:  0.6484090202928473 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  1 count:  3100 , train loss:  0.3707962697567156 , test loss:  0.7556529555602629 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  1 count:  3200 , train loss:  0.37338150186261854 , test loss:  0.7960393857922411 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  1 count:  3300 , train loss:  0.37580567777664664 , test loss:  0.9139847307742207 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  3400 , train loss:  0.3753688512647161 , test loss:  0.6560189103590522 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  3500 , train loss:  0.37489332800375497 , test loss:  0.945889045063908 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  3600 , train loss:  0.37490077813225614 , test loss:  0.8472887596875807 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  3700 , train loss:  0.3746996080269501 , test loss:  0.568860353338755 ,acc:  0.78\n",
      "acc: 40\n",
      "epoch:  1 count:  3800 , train loss:  0.3762275176039804 , test loss:  0.7375071145685763 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  1 count:  3900 , train loss:  0.3760521345458873 , test loss:  0.995438369139731 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  4000 , train loss:  0.37790399540094155 , test loss:  0.44564799950865563 ,acc:  0.82\n",
      "acc: 45\n",
      "epoch:  1 count:  4100 , train loss:  0.3768309545509259 , test loss:  0.5035086424017754 ,acc:  0.9\n",
      "acc: 39\n",
      "epoch:  1 count:  4200 , train loss:  0.37553898441136413 , test loss:  0.6847707817174842 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  4300 , train loss:  0.3732222908235954 , test loss:  0.6569129288953955 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  4400 , train loss:  0.37233549383035847 , test loss:  0.7227554959937328 ,acc:  0.8\n",
      "acc: 39\n",
      "epoch:  1 count:  4500 , train loss:  0.3713566732794983 , test loss:  0.8197044003951531 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  4600 , train loss:  0.37087061183831843 , test loss:  0.5585092040619464 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  1 count:  4700 , train loss:  0.37049354051171307 , test loss:  0.5196494119130238 ,acc:  0.84\n",
      "acc: 39\n",
      "epoch:  1 count:  4800 , train loss:  0.3704433546375752 , test loss:  0.5400603244527337 ,acc:  0.78\n",
      "acc: 39\n",
      "epoch:  1 count:  4900 , train loss:  0.3711919639791503 , test loss:  0.7227252219636625 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  5000 , train loss:  0.3712042244618899 , test loss:  0.7059513226556304 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  5100 , train loss:  0.37062907366396375 , test loss:  0.7648734583326078 ,acc:  0.8\n",
      "acc: 42\n",
      "epoch:  1 count:  5200 , train loss:  0.37134461238708205 , test loss:  0.6121944033456566 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  1 count:  5300 , train loss:  0.37094006761943316 , test loss:  0.5738220960989018 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  1 count:  5400 , train loss:  0.3715832514762695 , test loss:  0.5089082844834774 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  1 count:  5500 , train loss:  0.3709078098120597 , test loss:  0.5919410923804116 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  1 count:  5600 , train loss:  0.3705839438886697 , test loss:  0.7592599985138075 ,acc:  0.78\n",
      "acc: 41\n",
      "epoch:  1 count:  5700 , train loss:  0.3712523521415709 , test loss:  0.5277340534059977 ,acc:  0.82\n",
      "acc: 43\n",
      "epoch:  1 count:  5800 , train loss:  0.37120629343803313 , test loss:  0.5749668043300816 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  1 count:  5900 , train loss:  0.36909850740928996 , test loss:  0.5484601959196196 ,acc:  0.82\n",
      "acc: 39\n",
      "epoch:  1 count:  6000 , train loss:  0.3699581519104952 , test loss:  0.5909012087170867 ,acc:  0.78\n",
      "acc: 43\n",
      "epoch:  1 count:  6100 , train loss:  0.36897499692991786 , test loss:  0.562863476060976 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  1 count:  6200 , train loss:  0.3696645213879156 , test loss:  0.6703516676856259 ,acc:  0.86\n",
      "acc: 43\n",
      "epoch:  1 count:  6300 , train loss:  0.36786294378772283 , test loss:  0.5722171166453056 ,acc:  0.86\n",
      "acc: 40\n",
      "epoch:  1 count:  6400 , train loss:  0.3680001000039848 , test loss:  0.6209197808579505 ,acc:  0.8\n",
      "acc: 43\n",
      "epoch:  1 count:  6500 , train loss:  0.3687284673420435 , test loss:  0.4098058428694708 ,acc:  0.86\n",
      "acc: 41\n",
      "epoch:  1 count:  6600 , train loss:  0.3688621520714889 , test loss:  0.6044019965301595 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  6700 , train loss:  0.36893063107749124 , test loss:  0.5819061845446228 ,acc:  0.8\n",
      "acc: 44\n",
      "epoch:  1 count:  6800 , train loss:  0.36826904721835696 , test loss:  0.43275141190826505 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  1 count:  6900 , train loss:  0.3675713066772545 , test loss:  0.5201522856037147 ,acc:  0.84\n",
      "acc: 42\n",
      "epoch:  1 count:  7000 , train loss:  0.3681970517995585 , test loss:  0.4423911841392429 ,acc:  0.84\n",
      "acc: 37\n",
      "epoch:  1 count:  7100 , train loss:  0.36828159051953485 , test loss:  0.6485527657470083 ,acc:  0.74\n",
      "acc: 42\n",
      "epoch:  1 count:  7200 , train loss:  0.3683109771766009 , test loss:  0.4494399183403992 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  1 count:  7300 , train loss:  0.3694403665264613 , test loss:  0.489781284467731 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  1 count:  7400 , train loss:  0.36941635196908945 , test loss:  0.5462901496065772 ,acc:  0.84\n",
      "acc: 44\n",
      "epoch:  1 count:  7500 , train loss:  0.36903182044291893 , test loss:  0.4816404203746461 ,acc:  0.88\n",
      "acc: 42\n",
      "epoch:  1 count:  7600 , train loss:  0.36786194551244683 , test loss:  0.49027287250681184 ,acc:  0.84\n",
      "acc: 40\n",
      "epoch:  1 count:  7700 , train loss:  0.3688126609742633 , test loss:  0.4920025411742972 ,acc:  0.8\n",
      "acc: 41\n",
      "epoch:  1 count:  7800 , train loss:  0.36878507497010876 , test loss:  0.4577872579606445 ,acc:  0.82\n",
      "acc: 42\n",
      "epoch:  1 count:  7900 , train loss:  0.3687642784583981 , test loss:  0.5718615311467192 ,acc:  0.84\n",
      "acc: 41\n",
      "epoch:  1 count:  8000 , train loss:  0.36792468147666607 , test loss:  0.7020943241227724 ,acc:  0.82\n",
      "acc: 41\n",
      "epoch:  1 count:  8100 , train loss:  0.36766512212571045 , test loss:  0.558122026328856 ,acc:  0.82\n",
      "acc: 40\n",
      "epoch:  1 count:  8200 , train loss:  0.366598357948874 , test loss:  0.547019865928159 ,acc:  0.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 45\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     47\u001b[0m total_loss \u001b[38;5;241m=\u001b[39mtotal_loss \u001b[38;5;241m+\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def valid(model, test_data):\n",
    "    count = 0\n",
    "    acc = 0\n",
    "    total_loss = 0\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for features, target in test_data:\n",
    "        x = torch.tensor(features)\n",
    "        target = torch.tensor(target)\n",
    "        output = fnn(x)\n",
    "        loss = criterion(output, target)\n",
    "        output = softmax(output)\n",
    "        predict = torch.argmax(output, dim = -1)\n",
    "        count = count +1\n",
    "        total_loss = total_loss +loss.item()\n",
    "        if predict == target:\n",
    "            acc = acc +1\n",
    "    print(\"acc:\", acc)\n",
    "    return total_loss/count, acc/count\n",
    "feature_num = 13\n",
    "cluster_num = 100\n",
    "rule_num = cluster_num\n",
    "h = 1.0\n",
    "center,sigma = fcm(train_data, cluster_num= cluster_num, h= h)\n",
    "fnn = MLP(13, rule_num, center,sigma )\n",
    "optimizer = torch.optim.Adam(fnn.parameters(), lr=0.0001, weight_decay=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "## model parameters\n",
    "total_params = sum(p.numel() for p in fnn.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in fnn.parameters() if p.requires_grad)\n",
    "print(\"model total parameters: %d, trainable  parameters: %d \" %(total_params,total_trainable_params))\n",
    "epoch = 20\n",
    "for i in range(epoch):\n",
    "    count = 0\n",
    "    total_acc= 0\n",
    "    total_bleu=0\n",
    "    total_loss = 0\n",
    "    for features,target in train_data:\n",
    "        x = torch.tensor(features)\n",
    "        target = torch.tensor(target)\n",
    "        output = fnn(x)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count = count+ 1\n",
    "        total_loss =total_loss + loss.item()\n",
    "        if count %100 ==0:\n",
    "            test_loss, acc = valid(fnn, test_data)\n",
    "            print(\"epoch: \", i, \"count: \", count, \", train loss: \", total_loss/count, \", test loss: \", test_loss, \",acc: \",acc)\n",
    "            # print(\"epoch: \", i,\", count: \", count, \", train loss: \", total_loss/count)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
