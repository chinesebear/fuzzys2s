{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1548, 323, 358, 617, 264, 3221, 12, 15772, 2398, 292, 8830, 315, 1855, 1023, 13, 2360, 31373, 1587, 832, 315, 603, 2019, 2555, 1109, 279, 1023, 374, 2736, 30438, 13]\n",
      "He and I have a near-telepathic understanding of each other. No sooner does one of us say something than the other is already responding.\n",
      "[b'He', b' and', b' I', b' have', b' a', b' near', b'-', b'tele', b'path', b'ic', b' understanding', b' of', b' each', b' other', b'.', b' No', b' sooner', b' does', b' one', b' of', b' us', b' say', b' something', b' than', b' the', b' other', b' is', b' already', b' responding', b'.']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# 字节对编码过程，我的输出是[31373, 995]\n",
    "encoding_res = encoding.encode(\"He and I have a near-telepathic understanding of each other. No sooner does one of us say something than the other is already responding.\")\n",
    "print(encoding_res)\n",
    "# 字节对解码过程，解码结果：hello world\n",
    "raw_text = encoding.decode(encoding_res)\n",
    "print(raw_text)\n",
    "\n",
    "bpe = [encoding.decode_single_token_bytes(token) for token in encoding_res]\n",
    "print(bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['He', 'and', 'I', 'have', 'a', 'near', '-', 'tele', 'path', 'ic', 'understanding', 'of', 'each', 'other', '.', 'No', 'sooner', 'does', 'one', 'of', 'us', 'say', 'something', 'than', 'the', 'other', 'is', 'already', 'respon', 'ding', '.']\n",
      "He and I have a near - tele path ic understanding of each other . No sooner does one of us say something than the other is already respon ding .\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE,WordPiece,Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer,WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train(files=[\"./tatoeba/fra.txt\"], trainer=trainer)\n",
    "encoding_res = tokenizer.encode(\"He and I have a near-telepathic understanding of each other. No sooner does one of us say something than the other is already responding.\")\n",
    "print(encoding_res.tokens)\n",
    "decoding_res = tokenizer.decode(encoding_res.ids)\n",
    "print(decoding_res)\n",
    "\n",
    "# tokenizer.train_from_iterator(, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
